{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2441a734-880b-41a9-bf5d-aadfab171522",
      "metadata": {
        "id": "2441a734-880b-41a9-bf5d-aadfab171522"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2cb73e-78aa-405c-8bca-3eb25a8c3274",
      "metadata": {
        "id": "2a2cb73e-78aa-405c-8bca-3eb25a8c3274"
      },
      "outputs": [],
      "source": [
        "#merging the stopwords txt file\n",
        "\n",
        "StopWList=[]\n",
        "filelist=[\"C:/Users/Abhis/Desktop/black_coffer/20211030 Test Assignment-20220716T084424Z-001/20211030 Test Assignment/StopWords/StopWords_Auditor.txt\",\n",
        "          \"C:/Users/Abhis/Desktop/black_coffer/20211030 Test Assignment-20220716T084424Z-001/20211030 Test Assignment/StopWords/StopWords_Currencies.txt\",\n",
        "          \"C:/Users/Abhis/Desktop/black_coffer/20211030 Test Assignment-20220716T084424Z-001/20211030 Test Assignment/StopWords/StopWords_DatesandNumbers.txt\",\n",
        "          \"C:/Users/Abhis/Desktop/black_coffer/20211030 Test Assignment-20220716T084424Z-001/20211030 Test Assignment/StopWords/StopWords_Generic.txt\",\n",
        "          \"C:/Users/Abhis/Desktop/black_coffer/20211030 Test Assignment-20220716T084424Z-001/20211030 Test Assignment/StopWords/StopWords_GenericLong.txt\",\n",
        "          \"C:/Users/Abhis/Desktop/black_coffer/20211030 Test Assignment-20220716T084424Z-001/20211030 Test Assignment/StopWords/StopWords_Geographic.txt\",\n",
        "          \"C:/Users/Abhis/Desktop/black_coffer/20211030 Test Assignment-20220716T084424Z-001/20211030 Test Assignment/StopWords/StopWords_Names.txt\"]\n",
        "\n",
        "for i in range(len(filelist)):\n",
        "    \n",
        "    file=open(filelist[i],\"r\")\n",
        "\n",
        "    for word in file:\n",
        "    \n",
        "        stripped_word=word.strip()\n",
        "        head, sep, tail = stripped_word.partition('|')\n",
        "        stopword=re.sub('\\s',\"\",head)\n",
        "        \n",
        "        StopWList.append(stopword.lower())\n",
        "\n",
        "    file.close()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636e9347-5a30-4128-a958-6a1fbbb4d0ae",
      "metadata": {
        "id": "636e9347-5a30-4128-a958-6a1fbbb4d0ae"
      },
      "outputs": [],
      "source": [
        "textdf=pd.read_excel('C:/Users/Abhis/Desktop/black_coffer/soln/RawText_with_title.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71031a76-2de5-43b0-9dbb-9eabe7982b28",
      "metadata": {
        "id": "71031a76-2de5-43b0-9dbb-9eabe7982b28",
        "outputId": "ccd92f44-5a2e-42f9-83af-76913204233a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Abhis\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d33b377e-b10e-46e0-bd01-21e3a9690002",
      "metadata": {
        "id": "d33b377e-b10e-46e0-bd01-21e3a9690002"
      },
      "outputs": [],
      "source": [
        "#adding custom stopwords list to nltk's stopword\n",
        "stopword=nltk.corpus.stopwords.words('English')\n",
        "stopword.extend(StopWList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e8ae09-e232-4a79-9052-3bdb0fb4e279",
      "metadata": {
        "id": "a3e8ae09-e232-4a79-9052-3bdb0fb4e279",
        "outputId": "3838c47b-898f-4358-cf49-af393b4bebda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Abhis\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed31ddd2-a9e4-41eb-b3b4-6dd57cd86d73",
      "metadata": {
        "id": "ed31ddd2-a9e4-41eb-b3b4-6dd57cd86d73"
      },
      "outputs": [],
      "source": [
        "textdf['Text']=textdf['Text'].str.lower()\n",
        "textdf['Text'] = textdf['Text'].apply(lambda x: re.sub('[^A-Za-z0-9 ]', '', x)) #accepting only numbers and alphabets\n",
        "\n",
        "#tokenizing\n",
        "\n",
        "texttokendf=pd.DataFrame(columns=['Tokens'])\n",
        "\n",
        "final_text=[[]]*150\n",
        "\n",
        "for i in range(len(textdf)):\n",
        "    tokens=word_tokenize(textdf['Text'][i])\n",
        "    final_text[i]=[token for token in tokens if not token in stopword]\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1172ef62-890d-418d-bcc0-3ffafb31a810",
      "metadata": {
        "id": "1172ef62-890d-418d-bcc0-3ffafb31a810",
        "outputId": "4bd52fab-cf3e-4d9f-d02d-cc3838dc82df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(final_text) #check length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf127f5-f5bc-4fa5-908c-30b7d4cac3ec",
      "metadata": {
        "id": "acf127f5-f5bc-4fa5-908c-30b7d4cac3ec"
      },
      "outputs": [],
      "source": [
        "#coverting to df\n",
        "final_textdf=pd.Series(final_text).to_frame('words') #contains list of words (cleaned), textdf is uncleaned but wihtout special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac99d2cb-dd31-4908-98d0-fc59f5afe867",
      "metadata": {
        "id": "ac99d2cb-dd31-4908-98d0-fc59f5afe867"
      },
      "outputs": [],
      "source": [
        "negative_words=[]\n",
        "positive_words=[]\n",
        "\n",
        "sentiment_list=[negative_words,positive_words]\n",
        "\n",
        "#fetching positive and negative words .txt file -- pls change path\n",
        "filelistsentiment=[\"C:/Users/Abhis/Desktop/black_coffer/20211030 Test Assignment-20220716T084424Z-001/20211030 Test Assignment/MasterDictionary/negative-words.txt\",\n",
        "          \"C:/Users/Abhis/Desktop/black_coffer/20211030 Test Assignment-20220716T084424Z-001/20211030 Test Assignment/MasterDictionary/positive-words.txt\"]\n",
        "\n",
        "#converting .txt file to list\n",
        "for i in range(len(filelistsentiment)):\n",
        "    \n",
        "    file=open(filelistsentiment[i],\"r\")\n",
        "\n",
        "    for word in file:\n",
        "    \n",
        "        stripped_word=word.strip()\n",
        "        #head, sep, tail = stripped_word.partition('|')\n",
        "        #stopword=re.sub('\\s',\"\",head)\n",
        "        \n",
        "        sentiment_list[i].append(stripped_word)\n",
        "    file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50fc8339-fd78-46ab-b57e-3b73d0cf9ec2",
      "metadata": {
        "id": "50fc8339-fd78-46ab-b57e-3b73d0cf9ec2",
        "outputId": "9b22eca5-c132-4bf8-860e-c090b5a5ac70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4783 2006\n"
          ]
        }
      ],
      "source": [
        "print(len(negative_words),len(positive_words)) #check len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83af87b8-a055-4bd7-a7cf-97c5585a023b",
      "metadata": {
        "id": "83af87b8-a055-4bd7-a7cf-97c5585a023b"
      },
      "outputs": [],
      "source": [
        "#loop takes long time hence created intermediate outputdf which has positive and negative count in it\n",
        "for i in range(len(final_text)):\n",
        "    #count number of positive words\n",
        "     pos_count=final_textdf['words'].apply(lambda x: len([w for w in x if w in negative_words]))\n",
        "    #count number of negative words\n",
        "     neg_count=final_textdf['words'].apply(lambda x: len([w for w in x if w in positive_words]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1abc8cdf-d074-421a-8ccb-3c6b7e4b33ca",
      "metadata": {
        "id": "1abc8cdf-d074-421a-8ccb-3c6b7e4b33ca"
      },
      "outputs": [],
      "source": [
        "outputdf=pd.read_excel(r'C:\\Users\\Abhis\\Desktop\\black_coffer\\20211030 Test Assignment-20220716T084424Z-001\\20211030 Test Assignment\\Input.xlsx') #converting pinput file to df -- pls change path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e66fb8e5-7565-4ae3-8281-3e458ea820e2",
      "metadata": {
        "id": "e66fb8e5-7565-4ae3-8281-3e458ea820e2"
      },
      "outputs": [],
      "source": [
        "outputdf['POSITIVE COUNT']=pos_count\n",
        "outputdf['NEGATIVE COUNT']=neg_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d76ae52-a8be-4b1c-a4af-d9e6c0766303",
      "metadata": {
        "id": "6d76ae52-a8be-4b1c-a4af-d9e6c0766303"
      },
      "outputs": [],
      "source": [
        "outputdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fd51d6d-3ac9-4e29-8388-f5b0121f7d20",
      "metadata": {
        "id": "6fd51d6d-3ac9-4e29-8388-f5b0121f7d20"
      },
      "outputs": [],
      "source": [
        "#outputdf.to_excel(r'C:\\Users\\Abhis\\Desktop\\Output_with_count.xlsx') #saving progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "685734bd-e966-42c2-a46b-3f62ccaec405",
      "metadata": {
        "id": "685734bd-e966-42c2-a46b-3f62ccaec405"
      },
      "outputs": [],
      "source": [
        "#outputdf=pd.read_excel(r'C:\\Users\\Abhis\\Desktop\\Output_with_count.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd8c2b99-5f97-447d-b9dd-239fbe4b848e",
      "metadata": {
        "id": "cd8c2b99-5f97-447d-b9dd-239fbe4b848e"
      },
      "outputs": [],
      "source": [
        "\n",
        "#calculating polarity and subjectivity score based on pos and neg count \n",
        "\n",
        "totalword_count=[]\n",
        "for i in range(len(final_textdf)): \n",
        "    \n",
        "    totalword_count.append(len(final_textdf['words'][i]))\n",
        "    \n",
        "    polarity_score=(outputdf['POSITIVE COUNT']-outputdf['NEGATIVE COUNT'])/((outputdf['POSITIVE COUNT']+outputdf['NEGATIVE COUNT'])+0.000001)\n",
        "    \n",
        "    subjectivity_score=(outputdf['POSITIVE COUNT']+outputdf['NEGATIVE COUNT'])/((totalword_count[i])+0.000001)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc9c417-3191-4195-be9a-fe7b92636f0d",
      "metadata": {
        "id": "edc9c417-3191-4195-be9a-fe7b92636f0d",
        "outputId": "d981b4c6-e16a-4538-c34f-2534ae62abbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0     -0.714286\n",
            "1     -0.333333\n",
            "2     -0.494737\n",
            "3     -0.472222\n",
            "4     -0.411765\n",
            "         ...   \n",
            "145    0.111111\n",
            "146   -0.534884\n",
            "147    0.205882\n",
            "148   -0.789474\n",
            "149    0.090909\n",
            "Length: 150, dtype: float64 0      0.071138\n",
            "1      0.097561\n",
            "2      0.193089\n",
            "3      0.146341\n",
            "4      0.207317\n",
            "         ...   \n",
            "145    0.091463\n",
            "146    0.087398\n",
            "147    0.138211\n",
            "148    0.077236\n",
            "149    0.134146\n",
            "Length: 150, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print(polarity_score,subjectivity_score) #checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a6efe2-a32f-4032-a90c-4c73807fe035",
      "metadata": {
        "id": "d8a6efe2-a32f-4032-a90c-4c73807fe035"
      },
      "outputs": [],
      "source": [
        "text_punct=pd.read_excel(r'C:/Users/Abhis/Desktop/black_coffer/soln/RawText_with_title.xlsx') #converting a rawtext data with punctuations to df -- pls change path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d2b8ede-9fab-4122-9d7d-9549d4f4271f",
      "metadata": {
        "id": "9d2b8ede-9fab-4122-9d7d-9549d4f4271f",
        "outputId": "753b070f-ccd0-44c3-d09f-7945d1176cde"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is telehealth the future of healthcare  Covid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>how telehealth and telemedicine helping peopl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>is telemedicine effective in treating patient...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>is telehealth the future of healthcare  Telem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>how people diverted to telehealth services an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>145</td>\n",
              "      <td>blockchain for payments Reconciling with the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>146</td>\n",
              "      <td>the future of investing What Is an Investment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>147</td>\n",
              "      <td>big data analytics in healthcare Quality and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>148</td>\n",
              "      <td>business analytics in the healthcare industry...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>149</td>\n",
              "      <td>challenges and opportunities of big data in h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0                                               Text\n",
              "0             0   is telehealth the future of healthcare  Covid...\n",
              "1             1   how telehealth and telemedicine helping peopl...\n",
              "2             2   is telemedicine effective in treating patient...\n",
              "3             3   is telehealth the future of healthcare  Telem...\n",
              "4             4   how people diverted to telehealth services an...\n",
              "..          ...                                                ...\n",
              "145         145   blockchain for payments Reconciling with the ...\n",
              "146         146   the future of investing What Is an Investment...\n",
              "147         147   big data analytics in healthcare Quality and ...\n",
              "148         148   business analytics in the healthcare industry...\n",
              "149         149   challenges and opportunities of big data in h...\n",
              "\n",
              "[150 rows x 2 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_punct #checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad37fa9a-19eb-4906-8519-3c5dc1e8df3f",
      "metadata": {
        "id": "ad37fa9a-19eb-4906-8519-3c5dc1e8df3f",
        "outputId": "f223ebe8-9a20-453c-e37a-2da3d55f6a0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      [telehealth, future, healthcare, covid19, pave...\n",
              "1      [telehealth, telemedicine, helping, people, fi...\n",
              "2      [telemedicine, effective, treating, patients, ...\n",
              "3      [telehealth, future, healthcare, telemedicine,...\n",
              "4      [people, diverted, telehealth, services, telem...\n",
              "                             ...                        \n",
              "145    [blockchain, payments, reconciling, financial,...\n",
              "146    [future, investing, investmentan, investment, ...\n",
              "147    [big, data, analytics, healthcare, quality, af...\n",
              "148    [business, analytics, healthcare, industry, an...\n",
              "149    [challenges, opportunities, big, data, healthc...\n",
              "Name: words, Length: 150, dtype: object"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_textdf['words'] #checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe29d85a-8fb0-41a8-bddf-93f938b7d667",
      "metadata": {
        "id": "fe29d85a-8fb0-41a8-bddf-93f938b7d667"
      },
      "outputs": [],
      "source": [
        "#from nltk.corpus import cmudict\n",
        "\n",
        "#cmudic=cmudict.dict()\n",
        "\n",
        "sentence_count=[]\n",
        "word_count=[]\n",
        "avg_sentence_len=[]\n",
        "words=[]\n",
        "complex_count=[]\n",
        "percent_complex=[]\n",
        "fog_index=[]\n",
        "cleanword_count=[]\n",
        "total_slbl_count=[]\n",
        "slbl_per_word=[]\n",
        "pronoun_count=[]\n",
        "avg_word_len=[]\n",
        "\n",
        "\n",
        "avg_num_words=[]*150\n",
        "for i in range(len(textdf)):\n",
        "    \n",
        "    #calculating avg sentence length\n",
        "\n",
        "    lst=text_punct['Text'][i].split('.')\n",
        "    sentence_count.append(len(lst))\n",
        "    \n",
        "    avg_sentence_len.append(sum(len(x.split()) for x in lst) / len(lst))  \n",
        "    \n",
        "    #removing none in list to calculate word count\n",
        "\n",
        "    words=(textdf['Text'][i].split(\" \"))\n",
        "    twords=[z for z in words if z]\n",
        "    \n",
        "    word_count.append(len(twords))\n",
        "    \n",
        "    cleanword_count.append(len(final_textdf['words'][i]))\n",
        "    \n",
        "    #calculate avg_num_words\n",
        "\n",
        "    avg_num_words.append(word_count[i]/sentence_count[i])\n",
        "    \n",
        "    complexcount=0\n",
        "    totalcount=0\n",
        "    charcount=0\n",
        "    \n",
        "    #Calculating Syllables:\n",
        "\n",
        "    for j in range(len(twords)):\n",
        "        twords[j] = twords[j].lower()\n",
        "        count=0\n",
        "        vowels = \"aeiouy\"\n",
        "        if twords[j][0] in vowels:\n",
        "            count += 1\n",
        "            \n",
        "        for index in range(1, len(twords[j])):\n",
        "            \n",
        "            if twords[j][index] in vowels and twords[j][index - 1] not in vowels:\n",
        "                \n",
        "                count += 1\n",
        "                \n",
        "        if twords[j].endswith(\"e\") and not twords[j].endswith(\"le\"):\n",
        "            count -= 1\n",
        "            \n",
        "        if count == 0:\n",
        "            count += 1\n",
        "\n",
        "        #calculating count of complex words present:\n",
        "\n",
        "        if count > 2:\n",
        "            complexcount+=1\n",
        "            \n",
        "        totalcount+=count\n",
        "        \n",
        "        #calculating number of charcaters present in a given text list\n",
        "\n",
        "        charcount+=len(twords[j])\n",
        "        \n",
        "    #appending respective counts to their lists\n",
        "\n",
        "    complex_count.append(complexcount)\n",
        "    complexcount=0\n",
        "    \n",
        "    total_slbl_count.append(totalcount)\n",
        "    totalcount=0\n",
        "    \n",
        "    #calculate avg word length\n",
        "\n",
        "    avg_word_len.append(charcount/word_count[i])\n",
        "    charcount=0\n",
        "    \n",
        "    #calculate syllable per word:\n",
        "\n",
        "    slbl_per_word.append(total_slbl_count[i]/word_count[i])\n",
        "    \n",
        "    #calculate percentage of complex words\n",
        "\n",
        "    percent_complex.append(complex_count[i]/word_count[i])\n",
        "    \n",
        "    #calculate fog_index\n",
        "\n",
        "    fog_index.append(0.4 * (avg_sentence_len[i] + percent_complex[i]))\n",
        "    \n",
        "    #calculate personal pronouns count:\n",
        "\n",
        "    pronounRegex = re.compile(r'\\bi\\b|\\bwe\\b|\\bmy\\b|\\bours\\b|\\bus\\b')\n",
        "    pronouns=pronounRegex.findall(textdf['Text'][i])\n",
        "    pronoun_count.append(len(pronouns)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0e869ed-0971-41cf-8900-48a9cc17a638",
      "metadata": {
        "id": "f0e869ed-0971-41cf-8900-48a9cc17a638",
        "outputId": "4f6037a9-31b7-4979-dccf-adb5c496fe3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150 150 150 150 150 150 150 150 150 150 150 150 150\n"
          ]
        }
      ],
      "source": [
        "#checking lengths:\n",
        "print(len(sentence_count),len(word_count),len(avg_num_words),len(avg_sentence_len),len(complex_count),len(percent_complex), len(fog_index),len(cleanword_count),len(slbl_per_word),len(pronoun_count),len(avg_word_len),len(polarity_score),len(subjectivity_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c6ff5a-85b3-4660-bbb1-b0b5675883ae",
      "metadata": {
        "id": "f8c6ff5a-85b3-4660-bbb1-b0b5675883ae"
      },
      "outputs": [],
      "source": [
        "#adding the lists as columns for output.xlsx file\n",
        "\n",
        "Columns=['POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']\n",
        "Datas=[polarity_score,subjectivity_score,avg_sentence_len,percent_complex,fog_index,avg_num_words,complex_count,word_count,slbl_per_word,pronoun_count,avg_word_len]\n",
        "len(Columns)\n",
        "\n",
        "len(Datas)\n",
        "\n",
        "for column, data in zip(Columns,Datas):\n",
        "    outputdf[column]=data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86223ee3-42ca-42cb-bc41-4dfc291156ca",
      "metadata": {
        "id": "86223ee3-42ca-42cb-bc41-4dfc291156ca"
      },
      "outputs": [],
      "source": [
        "foutputdf=outputdf.drop(['Unnamed: 0'], axis=1)\n",
        "foutputdf.to_excel(r'C:\\Users\\Abhis\\Desktop\\Final Output.xlsx') #convert to excel -- pls change path"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}